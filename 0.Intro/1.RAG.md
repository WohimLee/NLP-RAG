# RAG
- [ACL 2023 Tutorial:
Retrieval-based Language Models and Applications](https://acl2023-retrieval-lm.github.io/)

## 1 大规模语言模型的现状及挑战
在自然语言处理领域, 大型语言模型（LLM）如GPT-3、BERT等已经取得了显著的进展, 它们能够生成连贯、自然的文本, 回答问题, 并执行其他复杂的语言任务。然而, 这些模型存在一些固有的局限性, 如 “模型幻觉问题”、“时效性问题”和“数据安全问题”。

大模型的幻觉指的是模型或产生大量不准确或误导性的输出，这种幻觉会导致大模型输出似是而非但实际不正确的答案，或者给出与上下文并不相关的输出。这种幻觉来的本质是由于大语言模型本身缺乏对于现实世界的感知能力，其训练数据可能存在偏见，不完整，错误虚假信息，训练中可能存在的过拟合和量化误差，以及Prompt上下文缺失等情况。OpenAI也提到：ChatGPT具有巨大的使用现有文档的能力，但它没有辨别真假的能力。它不是为此而建的。在某些关键任务中，幻觉的确可能带来严重的后果和误导。

当然，幻觉的存在不意味着大模型就无法在生产环境中落地。治理幻觉的方式有很多，包括在训练时提供更高质量的数据，对模型进行Finetune补充领域内知识，在RLHF给予Reward Model关于数据真实性更高的倾向性，通过Prompt引导大模型避免对缺乏信息的问题进行生成，以及本文所提到Retrieval Augment Generation，基于向量数据库的召回式生成。合理利用幻觉，可以充分发挥大模型的推理能力和创造能力，解决更加发散性的问题。

为了克服这些限制, 检索增强生成（RAG）技术应运而生。

- 多功能性和智能化：LLMs改变了我们与信息的互动方式
- 存在的问题：误导性的“幻觉”、信息过时、缺乏专业深度、推理能力欠缺等


## 2 RAG (检索增强生成技术)
Retrieval Augment Generation有两个重要的组成部分，`预训练大模型`和`领域知识库`。大模型时代，AI的应用发生了新的范式变化，从过往的`预训练+Finetune`的模式转换为了`预训练+Prompt`，这大大简化了对于不同任务训练模型的工作量，降低了AI的开发和使用门槛，也把搜索结合生成成为了可能。

在RAG架构中，预训练LLM往往有以下几个特点：

- 基于Transformer架构，准确捕获句子之间的相关性
- 基于预测上下文的Token任务进行预训练，同时结合RLHF技术使得生成结果更加符合人的喜好
- 规模较大，往往需要百亿甚至千亿级别参数，泛化能力较强

另一方面，对领域知识的补充往往依赖于特定的数据存储，如向量数据库，搜索引擎或者图数据库。这其中，向量数据库因为具备对高维Embedding的检索能力，跟大模型的结合最为简单，效果也比较出色，目前是RAG中最为常用的数据存储方式。


<div align=center>
    <image src="imgs/RAG.png" width=800>
</div>

## 3 为什么需要 RAG

单单大模型，其实已经回答很多问题，而Finetune也可以起到补充领域知识的作用，为什么RAG仍然如此重要呢？

>LLM是有损压缩，不能记住其参数中的所有（长尾）知识
- 大模型尽管参数量很大，但相比人类的所有知识依然只占九牛一毛。因此例如问及 Geoffery Hinton 的文章时，最有名的几篇文章确实能够正确回答，但很多长尾的文章实际并非来自于Geoffery Hinton，甚至可能是捏造的。因此，大模型确实需要通过搜索召回相关领域的知识来补充领域特定的知识

>LLM的知识很容易过时，很难更新
- 大模型的训练数据存在时间截止的问题。尽管可以通过 Finetune 来为大模型加入新的知识，但大模型的的训练成本和时间依然是相当可观的，通常需要大量的计算资源，时间也是天级别更新。不论是向量数据库还是搜索引擎，数据的更新都更加容易，这有助于业务数据的实时性，且模型重新训练的周期被大大拉长了，避免了频繁Finetune带来的数据正确性，政策等风险

>LLM的输出很难解释和验证
- 使用 LLM 生成的结果，很难验证其准确性，存在着大量谬误。通过RAG的方式，所有数据可以记录数据来源，以便于通过人工或者机器的方式校验结果


>LLMs被证明很容易泄漏私有数据
- 尽管可以向 LLM 通过 Finetune 的方式添加领域知识，但这些领域知识很可能包含个人或者公司的机密信息，且这些数据很可能通过模型在不经意之间泄露出去。通过增加私有数据存储的方式，用户的数据可以更加安全